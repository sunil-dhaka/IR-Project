# Resources to help implement project
- [a nice git repo to help](https://github.com/icoxfog417/awesome-text-summarization)
- papers:
	- [Textrank: Brining order to the text](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)
    - [Text summarization using Latent Semantic Analysis](https://www.researchgate.net/publication/220195824_Text_summarization_using_Latent_Semantic_Analysis)
    - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
    - [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777)
- datasets:
    - [English corpora](https://github.com/Alex-Fabbri/Multi-News)
    - [Hindi corpora](https://www.kaggle.com/datasets/disisbig/hindi-text-short-and-large-summarization-corpus)
- [other resources for project](IR-Project/Resouces_for_IR_project.pdf)

For very concise summaries, [ROUGE-1](https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460) alone may suffice especially if you are also applying stemming and stop word removal.
Specific model running instructions and other relevant infos are given in their readme files.

# folder structure
```
.
├── course-ppt
│   ├── IR-textrank-ppt.pdf
│   ├── text-rank-ppt.pdf
│   └── textrank.xopp
├── group14_project
│   ├── data
│   ├── data.md
│   ├── flask-demo
│   ├── group-info.txt
│   ├── lda_textrank
│   ├── LSA
│   ├── pegasus_model
│   ├── ppt
│   ├── report
│   ├── t5_small
│   └── text-rank
├── group14_project.zip
├── LICENSE
├── project-idea
│   ├── group-idea-cs657a.pptx
│   └── project-idea.pdf
├── README.md
└── Resouces_for_IR_project.pdf
```