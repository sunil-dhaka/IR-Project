{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import modules\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import networkx as nx\n",
    "from networkx.exception import PowerIterationFailedConvergence\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nto download english/hindi glove embeddings\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "to download english/hindi glove embeddings\n",
    "\"\"\"\n",
    "# if want to do for other language then its vocab word embeddings will be needed. here is for english\n",
    "# ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# unzip archive file\n",
    "# ! unzip glove.6B.zip\n",
    "# ! wget https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/hi.tar.xz\n",
    "# ! tar -xvf hi.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FIXED VARIABLES DECLARED\n",
    "\"\"\"\n",
    "EMBEDDING_SIZE=100 # OPTIONS: 50,100,200,300\n",
    "SENT_SUMMARY_COUNT=5\n",
    "SENT_SUMMARY_THERSHOLD=0.35\n",
    "HI_DATA_PATH='../data/hi-data/test.csv' # BOTH ARTICLES AND SUMMARIES\n",
    "EN_DATA_PATH_SRC='../data/preprocessed_truncated/test.txt.src.tokenized.fixed.cleaned.final.truncated.txt' # ONLY ARTICLES\n",
    "EN_DATA_PATH_TGT='../data/preprocessed_truncated/test.txt.tgt.tokenized.fixed.cleaned.final.truncated.txt' # ONLY SUMMARIES\n",
    "HI_EMBEDDINGS_PATH='../data/glove/hi/hi-d100-glove.txt'\n",
    "EN_EMBEDDINGS_PATH='../data/glove/en/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_score(a,b):\n",
    "    \"\"\"\n",
    "    implementation of sent score mentioned in the paper\n",
    "    \"\"\"\n",
    "    words_a=a.split(' ')\n",
    "    words_b=b.split(' ')\n",
    "    # overlapping unigrams found in both sentences\n",
    "    count=0\n",
    "    for i in words_a:\n",
    "        if i in words_b:\n",
    "            count+=1\n",
    "\n",
    "    score=count/(len(words_a)*len(words_b))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 400000 words loaded from glove model, each of size 100.\n",
      "total 1732951 words loaded from glove model, each of size 100.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "HELPER FUNCTIONS FOR MAIN SUMMARY GENERATION FUNC\n",
    "\"\"\"\n",
    "def cosine_similarity(a,b):\n",
    "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def get_sent_vector(sentence):\n",
    "    global glove_embeddings\n",
    "\n",
    "    if len(sentence)!=0:\n",
    "        sentence_vect=sum([glove_embeddings.get(word,np.zeros((EMBEDDING_SIZE,))) for word in sentence.split()])/(len(sentence.split())) # +0.001) # 0.001 to make things stable\n",
    "    else:\n",
    "        sentence_vect=np.zeros((EMBEDDING_SIZE,))\n",
    "    return sentence_vect\n",
    "\n",
    "def load_glove_model(file_path):\n",
    "    \"\"\"\n",
    "    function to load glove embeddings from txt file into a dict\n",
    "    \"\"\"\n",
    "    glove_model = {}\n",
    "    with open(file_path,'r',encoding=\"utf-8\",errors='ignore') as f:\n",
    "        try:\n",
    "            for i,line in enumerate(f):\n",
    "                # if i%10000==0:\n",
    "                #     print(i)\n",
    "                split_line = line.split()\n",
    "                word = split_line[0]\n",
    "                embedding = np.array(split_line[-EMBEDDING_SIZE:], dtype=np.float64)\n",
    "                glove_model[word] = embedding\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(line,i)\n",
    "    print(f\"total {len(glove_model)} words loaded from glove model, each of size {EMBEDDING_SIZE}.\")\n",
    "    return glove_model\n",
    "\n",
    "# load glove embeddings for both hindi and english\n",
    "glove_embeddings=load_glove_model(file_path=EN_EMBEDDINGS_PATH)\n",
    "hi_glove_embeddings=load_glove_model(file_path=HI_EMBEDDINGS_PATH)\n",
    "# save all glove embeddings in a single dict\n",
    "# this takes care of both languages that are present in hindi texts; although it makes overall process a bit slow\n",
    "glove_embeddings.update(hi_glove_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning params\n",
    "- embedding size\n",
    "- rather than fixing number of sents to consider in summary, should decide by a thershold value >> possibly different for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(article_text):\n",
    "    \"\"\"\n",
    "    function to get summary of article text\n",
    "    note: article text needs to be pre-processed; and that step only includes: string.lower()\n",
    "    \"\"\"\n",
    "    # pre-process\n",
    "    article_text=article_text.lower()\n",
    "    # tokenize into sentences\n",
    "    sent_tokens=sent_tokenize(article_text)\n",
    "    # get sent vectores\n",
    "    sent_vects=[]\n",
    "    for sent in sent_tokens:\n",
    "        sent_vects.append(get_sent_vector(sent))\n",
    "    \n",
    "    total_sents=len(sent_tokens)\n",
    "    similarity_mat=np.zeros((total_sents,total_sents))\n",
    "    for i in range(total_sents):\n",
    "        for j in range(total_sents):\n",
    "            # this way diag entries will be zeros\n",
    "            if i!=j:\n",
    "                similarity_mat[i][j]=cosine_similarity(sent_vects[i],sent_vects[j])\n",
    "    \n",
    "    # create graph from similarity matrix\n",
    "    network_graph=nx.from_numpy_array(similarity_mat)\n",
    "    \"\"\"\n",
    "    apply pagerank algo to get scores\n",
    "    \"\"\"\n",
    "    # PowerIterationFailedConvergence: (PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')\n",
    "    # to solve convergence iteration problem can change max_iter=100(default)\n",
    "    # isConverged=False\n",
    "    # iter_count=100\n",
    "    # while(True):\n",
    "    #     print(iter_count)\n",
    "    #     if not isConverged:\n",
    "    #         try:\n",
    "    #             scores=nx.pagerank_numpy(network_graph, alpha=0.85, max_iter=iter_count)\n",
    "    #             isConverged=True\n",
    "    #         except PowerIterationFailedConvergence:\n",
    "    #             iter_count*=10\n",
    "    #     else:\n",
    "    #         break\n",
    "    scores=nx.pagerank(network_graph, alpha=0.85, max_iter=1000)\n",
    "    sorted_scores = sorted(((scores[i],sent_token) for i,sent_token in enumerate(sent_tokens)), reverse=True)\n",
    "\n",
    "    # TODO: implement thershold version\n",
    "    # in case some article does not have enough sentences\n",
    "    if total_sents > SENT_SUMMARY_COUNT:\n",
    "        summary_text=' '.join([sorted_scores[i][1] for i in range(SENT_SUMMARY_COUNT)])\n",
    "    else:\n",
    "        summary_text=' '.join([sorted_scores[i][1] for i in range(total_sents)])\n",
    "\n",
    "    return summary_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunild/anaconda3/envs/ir/lib/python3.9/site-packages/networkx/algorithms/link_analysis/pagerank_alg.py:108: DeprecationWarning: networkx.pagerank_scipy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n",
      "  return pagerank_scipy(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a fresh update on the u.s. employment situation for january hits the wires at 8 : 30 a.m. new york time offering one of the most important snapshots on how the economy fared during the previous month . it ’ s the first friday of the month , when for one ever-so-brief moment the interests of wall street , washington and main street are all aligned on one thing : jobs . the unemployment rate dipped , but mostly because more americans stopped looking for work . the labor department says the economy added 120,000 jobs in march , down from more than 200,000 in each of the previous three months . the economy has added 858,000 jobs since december _ the best four months of hiring in two years .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test case for generate_summary func\n",
    "\"\"\"\n",
    "test_text=\"national archives     yes , it ’ s that time again , folks . it ’ s the first friday of the month , when for one ever-so-brief moment the interests of wall street , washington and main street are all aligned on one thing : jobs .     a fresh update on the u.s. employment situation for january hits the wires at 8 : 30 a.m. new york time offering one of the most important snapshots on how the economy fared during the previous month . expectations are for 203,000 new jobs to be created , according to economists polled by dow jones newswires , compared to 227,000 jobs added in february . the unemployment rate is expected to hold steady at 8.3 % .     here at marketbeat hq , we ’ ll be offering color commentary before and after the data crosses the wires . feel free to weigh-in yourself , via the comments section . and while you ’ re here , why don ’ t you sign up to  .     enjoy the show . story_separator_special_tag employers pulled back sharply on hiring last month , a reminder that the u.s. economy may not be growing fast enough to sustain robust job growth . the unemployment rate dipped , but mostly because more americans stopped looking for work .     the labor department says the economy added 120,000 jobs in march , down from more than 200,000 in each of the previous three months .     the unemployment rate fell to 8.2 percent , the lowest since january 2009. the rate dropped because fewer people searched for jobs . the official unemployment tally only includes those seeking work .     the economy has added 858,000 jobs since december _ the best four months of hiring in two years . but federal reserve chairman ben bernanke has cautioned that the current hiring pace is unlikely to continue without more consumer spending .\"\n",
    "generate_summary(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_data=pd.read_csv('../data/hi-data/test.csv',lineterminator='\\n')\n",
    "hi_data.drop('headline',axis=1,inplace=True)\n",
    "hi_data.dropna(inplace=True)\n",
    "hi_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunild/anaconda3/envs/ir/lib/python3.9/site-packages/networkx/algorithms/link_analysis/pagerank_alg.py:108: DeprecationWarning: networkx.pagerank_scipy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n",
      "  return pagerank_scipy(\n",
      "/tmp/ipykernel_10872/345323954.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunild/anaconda3/envs/ir/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/sunild/anaconda3/envs/ir/lib/python3.9/site-packages/networkx/algorithms/link_analysis/pagerank_alg.py:505: RuntimeWarning: invalid value encountered in subtract\n",
      "  err = np.absolute(x - xlast).sum()\n",
      "/home/sunild/anaconda3/envs/ir/lib/python3.9/site-packages/networkx/algorithms/link_analysis/pagerank_alg.py:505: RuntimeWarning: overflow encountered in subtract\n",
      "  err = np.absolute(x - xlast).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "------\n",
      "300\n",
      "------\n",
      "400\n",
      "------\n",
      "500\n",
      "------\n",
      "600\n",
      "------\n",
      "700\n",
      "------\n",
      "800\n",
      "------\n",
      "900\n",
      "------\n",
      "1000\n",
      "------\n",
      "1100\n",
      "------\n",
      "1200\n",
      "------\n",
      "1300\n",
      "------\n",
      "1400\n",
      "------\n",
      "1500\n",
      "------\n",
      "1600\n",
      "------\n",
      "1700\n",
      "------\n",
      "1800\n",
      "------\n",
      "1900\n",
      "------\n",
      "2000\n",
      "------\n",
      "2100\n",
      "------\n",
      "2200\n",
      "------\n",
      "2300\n",
      "------\n",
      "2400\n",
      "------\n",
      "2500\n",
      "------\n",
      "2600\n",
      "------\n",
      "2700\n",
      "------\n",
      "2800\n",
      "------\n",
      "2900\n",
      "------\n",
      "3000\n",
      "------\n",
      "3100\n",
      "------\n",
      "3200\n",
      "------\n",
      "3300\n",
      "------\n",
      "3400\n",
      "------\n",
      "3500\n",
      "------\n",
      "3600\n",
      "------\n",
      "3700\n",
      "------\n",
      "3800\n",
      "------\n",
      "3900\n",
      "------\n",
      "4000\n",
      "------\n",
      "4100\n",
      "------\n",
      "4200\n",
      "------\n",
      "4300\n",
      "------\n",
      "4400\n",
      "------\n",
      "4500\n",
      "------\n",
      "4600\n",
      "------\n",
      "4700\n",
      "------\n",
      "4800\n",
      "------\n",
      "4900\n",
      "------\n",
      "5000\n",
      "------\n",
      "5100\n",
      "------\n",
      "5200\n",
      "------\n",
      "5300\n",
      "------\n",
      "5400\n",
      "------\n",
      "5500\n",
      "------\n",
      "5600\n",
      "------\n",
      "5700\n",
      "------\n",
      "5800\n",
      "------\n",
      "5900\n",
      "------\n",
      "6000\n",
      "------\n",
      "6100\n",
      "------\n",
      "6200\n",
      "------\n",
      "6300\n",
      "------\n",
      "6400\n",
      "------\n",
      "6500\n",
      "------\n",
      "6600\n",
      "------\n",
      "6700\n",
      "------\n",
      "6800\n",
      "------\n",
      "6900\n",
      "------\n",
      "7000\n",
      "------\n",
      "7100\n",
      "------\n",
      "7200\n",
      "------\n",
      "7300\n",
      "------\n",
      "7400\n",
      "------\n",
      "7500\n",
      "------\n",
      "7600\n",
      "------\n",
      "7700\n",
      "------\n",
      "7800\n",
      "------\n",
      "7900\n",
      "------\n",
      "8000\n",
      "------\n",
      "8100\n",
      "------\n",
      "8200\n",
      "------\n",
      "8300\n",
      "------\n",
      "8400\n",
      "------\n",
      "8500\n",
      "------\n",
      "8600\n",
      "------\n",
      "8700\n",
      "------\n",
      "8800\n",
      "------\n",
      "8900\n",
      "------\n",
      "9000\n",
      "------\n",
      "9100\n",
      "------\n",
      "9200\n",
      "------\n",
      "9300\n",
      "------\n",
      "9400\n",
      "------\n",
      "9500\n",
      "------\n",
      "9600\n",
      "------\n",
      "9700\n",
      "------\n",
      "9800\n",
      "------\n",
      "9900\n",
      "------\n",
      "10000\n",
      "------\n",
      "10100\n",
      "------\n",
      "10200\n",
      "------\n",
      "10300\n",
      "------\n",
      "10400\n",
      "------\n",
      "10500\n",
      "------\n",
      "10600\n",
      "------\n",
      "10700\n",
      "------\n",
      "10800\n",
      "------\n",
      "10900\n",
      "------\n",
      "11000\n",
      "------\n",
      "11100\n",
      "------\n",
      "11200\n",
      "------\n",
      "11300\n",
      "------\n",
      "11400\n",
      "------\n",
      "11500\n",
      "------\n",
      "11600\n",
      "------\n",
      "11700\n",
      "------\n",
      "11800\n",
      "------\n",
      "11900\n",
      "------\n",
      "12000\n",
      "------\n",
      "12100\n",
      "------\n",
      "12200\n",
      "------\n",
      "12300\n",
      "------\n",
      "12400\n",
      "------\n",
      "12500\n",
      "------\n",
      "12600\n",
      "------\n",
      "12700\n",
      "------\n",
      "12800\n",
      "------\n",
      "12900\n",
      "------\n",
      "13000\n",
      "------\n",
      "13100\n",
      "------\n",
      "13200\n",
      "------\n",
      "13300\n",
      "------\n",
      "13400\n",
      "------\n",
      "13500\n",
      "------\n",
      "13600\n",
      "------\n",
      "13700\n",
      "------\n",
      "13800\n",
      "------\n",
      "13900\n",
      "------\n",
      "14000\n",
      "------\n",
      "14100\n",
      "------\n",
      "14200\n",
      "------\n",
      "14300\n",
      "------\n",
      "14400\n",
      "------\n",
      "14500\n",
      "------\n",
      "14600\n",
      "------\n",
      "14700\n",
      "------\n",
      "14800\n",
      "------\n",
      "14900\n",
      "------\n",
      "15000\n",
      "------\n",
      "15100\n",
      "------\n",
      "15200\n",
      "------\n",
      "15300\n",
      "------\n",
      "15400\n",
      "------\n",
      "15500\n",
      "------\n",
      "15600\n",
      "------\n",
      "15700\n",
      "------\n",
      "15800\n",
      "------\n",
      "15900\n",
      "------\n",
      "16000\n",
      "------\n",
      "16100\n",
      "------\n",
      "16200\n",
      "------\n",
      "16300\n",
      "------\n",
      "16400\n",
      "------\n",
      "16500\n",
      "------\n",
      "16600\n",
      "------\n",
      "16700\n",
      "------\n",
      "16800\n",
      "------\n",
      "16900\n",
      "------\n",
      "17000\n",
      "------\n",
      "17100\n",
      "------\n",
      "17200\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate summaries for hindi test dataset\n",
    "\"\"\"\n",
    "article_summary_list_hi=[]\n",
    "bad_articles_hi=[]\n",
    "for i,line in enumerate(list(hi_data['article'])):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        print(\"------\")\n",
    "    try:\n",
    "        article_summary_list_hi.append(generate_summary(line.strip()))\n",
    "    except PowerIterationFailedConvergence:\n",
    "        bad_articles_hi.append(i)\n",
    "        continue\n",
    "\n",
    "with open(f'../data/hi_article_summary_{EMBEDDING_SIZE}.txt','w') as file:\n",
    "    file.write('\\n'.join(article_summary_list_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "remove bad articles that does not have sufficient text data \n",
    "\"\"\"\n",
    "foo_refs=list(hi_data['summary'])\n",
    "hi_refs=[]\n",
    "for i,article in enumerate(foo_refs):\n",
    "    if i not in bad_articles_hi:\n",
    "        hi_refs.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "------\n",
      "100\n",
      "------\n",
      "200\n",
      "------\n",
      "300\n",
      "------\n",
      "400\n",
      "------\n",
      "500\n",
      "------\n",
      "600\n",
      "------\n",
      "700\n",
      "------\n",
      "800\n",
      "------\n",
      "900\n",
      "------\n",
      "1000\n",
      "------\n",
      "1100\n",
      "------\n",
      "1200\n",
      "------\n",
      "1300\n",
      "------\n",
      "1400\n",
      "------\n",
      "1500\n",
      "------\n",
      "1600\n",
      "------\n",
      "1700\n",
      "------\n",
      "1800\n",
      "------\n",
      "1900\n",
      "------\n",
      "2000\n",
      "------\n",
      "2100\n",
      "------\n",
      "2200\n",
      "------\n",
      "2300\n",
      "------\n",
      "2400\n",
      "------\n",
      "2500\n",
      "------\n",
      "2600\n",
      "------\n",
      "2700\n",
      "------\n",
      "2800\n",
      "------\n",
      "2900\n",
      "------\n",
      "3000\n",
      "------\n",
      "3100\n",
      "------\n",
      "3200\n",
      "------\n",
      "3300\n",
      "------\n",
      "3400\n",
      "------\n",
      "3500\n",
      "------\n",
      "3600\n",
      "------\n",
      "3700\n",
      "------\n",
      "3800\n",
      "------\n",
      "3900\n",
      "------\n",
      "4000\n",
      "------\n",
      "4100\n",
      "------\n",
      "4200\n",
      "------\n",
      "4300\n",
      "------\n",
      "4400\n",
      "------\n",
      "4500\n",
      "------\n",
      "4600\n",
      "------\n",
      "4700\n",
      "------\n",
      "4800\n",
      "------\n",
      "4900\n",
      "------\n",
      "5000\n",
      "------\n",
      "5100\n",
      "------\n",
      "5200\n",
      "------\n",
      "5300\n",
      "------\n",
      "5400\n",
      "------\n",
      "5500\n",
      "------\n",
      "5600\n",
      "------\n",
      "5700\n",
      "------\n",
      "5800\n",
      "------\n",
      "5900\n",
      "------\n",
      "6000\n",
      "------\n",
      "6100\n",
      "------\n",
      "6200\n",
      "------\n",
      "6300\n",
      "------\n",
      "6400\n",
      "------\n",
      "6500\n",
      "------\n",
      "6600\n",
      "------\n",
      "6700\n",
      "------\n",
      "6800\n",
      "------\n",
      "6900\n",
      "------\n",
      "7000\n",
      "------\n",
      "7100\n",
      "------\n",
      "7200\n",
      "------\n",
      "7300\n",
      "------\n",
      "7400\n",
      "------\n",
      "7500\n",
      "------\n",
      "7600\n",
      "------\n",
      "7700\n",
      "------\n",
      "7800\n",
      "------\n",
      "7900\n",
      "------\n",
      "8000\n",
      "------\n",
      "8100\n",
      "------\n",
      "8200\n",
      "------\n",
      "8300\n",
      "------\n",
      "8400\n",
      "------\n",
      "8500\n",
      "------\n",
      "8600\n",
      "------\n",
      "8700\n",
      "------\n",
      "8800\n",
      "------\n",
      "8900\n",
      "------\n",
      "9000\n",
      "------\n",
      "9100\n",
      "------\n",
      "9200\n",
      "------\n",
      "9300\n",
      "------\n",
      "9400\n",
      "------\n",
      "9500\n",
      "------\n",
      "9600\n",
      "------\n",
      "9700\n",
      "------\n",
      "9800\n",
      "------\n",
      "9900\n",
      "------\n",
      "10000\n",
      "------\n",
      "10100\n",
      "------\n",
      "10200\n",
      "------\n",
      "10300\n",
      "------\n",
      "10400\n",
      "------\n",
      "10500\n",
      "------\n",
      "10600\n",
      "------\n",
      "10700\n",
      "------\n",
      "10800\n",
      "------\n",
      "10900\n",
      "------\n",
      "11000\n",
      "------\n",
      "11100\n",
      "------\n",
      "11200\n",
      "------\n",
      "11300\n",
      "------\n",
      "11400\n",
      "------\n",
      "11500\n",
      "------\n",
      "11600\n",
      "------\n",
      "11700\n",
      "------\n",
      "11800\n",
      "------\n",
      "11900\n",
      "------\n",
      "12000\n",
      "------\n",
      "12100\n",
      "------\n",
      "12200\n",
      "------\n",
      "12300\n",
      "------\n",
      "12400\n",
      "------\n",
      "12500\n",
      "------\n",
      "12600\n",
      "------\n",
      "12700\n",
      "------\n",
      "12800\n",
      "------\n",
      "12900\n",
      "------\n",
      "13000\n",
      "------\n",
      "13100\n",
      "------\n",
      "13200\n",
      "------\n",
      "13300\n",
      "------\n",
      "13400\n",
      "------\n",
      "13500\n",
      "------\n",
      "13600\n",
      "------\n",
      "13700\n",
      "------\n",
      "13800\n",
      "------\n",
      "13900\n",
      "------\n",
      "14000\n",
      "------\n",
      "14100\n",
      "------\n",
      "14200\n",
      "------\n",
      "14300\n",
      "------\n",
      "14400\n",
      "------\n",
      "14500\n",
      "------\n",
      "14600\n",
      "------\n",
      "14700\n",
      "------\n",
      "14800\n",
      "------\n",
      "14900\n",
      "------\n",
      "15000\n",
      "------\n",
      "15100\n",
      "------\n",
      "15200\n",
      "------\n",
      "15300\n",
      "------\n",
      "15400\n",
      "------\n",
      "15500\n",
      "------\n",
      "15600\n",
      "------\n",
      "15700\n",
      "------\n",
      "15800\n",
      "------\n",
      "15900\n",
      "------\n",
      "16000\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate rouge score for hindi test dataset\n",
    "\"\"\"\n",
    "rouge_=Rouge()\n",
    "rouge_scores=[]\n",
    "bad_rouges=[]\n",
    "for i,_ in enumerate(hi_refs):\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        print(\"------\")\n",
    "    if i not in bad_articles_hi:\n",
    "        try:\n",
    "            rouge_scores.append(rouge_.get_scores(hyps=article_summary_list_hi[i],refs=hi_refs[i]))\n",
    "        except RecursionError:\n",
    "            bad_rouges.append(i)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg f1 score for rouge-1 measure is : 0.2926229192009566\n"
     ]
    }
   ],
   "source": [
    "rouge_1=[]\n",
    "for score in rouge_scores:\n",
    "    rouge_1.append(score[0]['rouge-1']['f'])\n",
    "print('avg f1 score for rouge-1 measure is :',sum(rouge_1)/len(rouge_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10872/345323954.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "------\n",
      "200\n",
      "------\n",
      "300\n",
      "------\n",
      "400\n",
      "------\n",
      "500\n",
      "------\n",
      "600\n",
      "------\n",
      "700\n",
      "------\n",
      "800\n",
      "------\n",
      "900\n",
      "------\n",
      "1000\n",
      "------\n",
      "1100\n",
      "------\n",
      "1200\n",
      "------\n",
      "1300\n",
      "------\n",
      "1400\n",
      "------\n",
      "1500\n",
      "------\n",
      "1600\n",
      "------\n",
      "1700\n",
      "------\n",
      "1800\n",
      "------\n",
      "1900\n",
      "------\n",
      "2000\n",
      "------\n",
      "2100\n",
      "------\n",
      "2200\n",
      "------\n",
      "2300\n",
      "------\n",
      "2400\n",
      "------\n",
      "2500\n",
      "------\n",
      "2600\n",
      "------\n",
      "2700\n",
      "------\n",
      "2800\n",
      "------\n",
      "2900\n",
      "------\n",
      "3000\n",
      "------\n",
      "3100\n",
      "------\n",
      "3200\n",
      "------\n",
      "3300\n",
      "------\n",
      "3400\n",
      "------\n",
      "3500\n",
      "------\n",
      "3600\n",
      "------\n",
      "3700\n",
      "------\n",
      "3800\n",
      "------\n",
      "3900\n",
      "------\n",
      "4000\n",
      "------\n",
      "4100\n",
      "------\n",
      "4200\n",
      "------\n",
      "4300\n",
      "------\n",
      "4400\n",
      "------\n",
      "4500\n",
      "------\n",
      "4600\n",
      "------\n",
      "4700\n",
      "------\n",
      "4800\n",
      "------\n",
      "4900\n",
      "------\n",
      "5000\n",
      "------\n",
      "5100\n",
      "------\n",
      "5200\n",
      "------\n",
      "5300\n",
      "------\n",
      "5400\n",
      "------\n",
      "5500\n",
      "------\n",
      "5600\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate summaries for english test dataset\n",
    "\"\"\"\n",
    "article_summary_list=[]\n",
    "bad_articles=[]\n",
    "with open(EN_DATA_PATH_SRC,'r') as file:\n",
    "    for i,line in enumerate(file):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "            print(\"------\")\n",
    "        try:\n",
    "            article_summary_list.append(generate_summary(line.strip()))\n",
    "        except PowerIterationFailedConvergence:\n",
    "            bad_articles.append(i)\n",
    "            continue\n",
    "\n",
    "with open(f'../data/en_article_summary_{EMBEDDING_SIZE}.txt','w') as file:\n",
    "    file.write('\\n'.join(article_summary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_summary=[]\n",
    "with open(EN_DATA_PATH_TGT,'r') as file:\n",
    "    for i,line in enumerate(file):\n",
    "        if i not in bad_articles:\n",
    "            ref_summary.append(line[2:].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 3 mins\n",
    "rouge_scores_en=rouge_.get_scores(hyps=article_summary_list,refs=ref_summary,avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.348989091353505,\n",
       "  'p': 0.4146895458675804,\n",
       "  'f': 0.3700004523905176},\n",
       " 'rouge-2': {'r': 0.12301443492634224,\n",
       "  'p': 0.14943295556465142,\n",
       "  'f': 0.13036603766465277},\n",
       " 'rouge-l': {'r': 0.2998443484438995,\n",
       "  'p': 0.3569296387214579,\n",
       "  'f': 0.3181192414432147}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores_en"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d66a3fad9284895a61a090a035e4fcbf146100e906c5a916a78dc9cbee9e65c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ir')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
